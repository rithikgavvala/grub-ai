{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Grub AI Notebook\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "import textblob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "reader = pd.read_json(\"data/yelp_academic_dataset_review.json\", lines=True, chunksize=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dataframe\n",
    "review_df = None\n",
    "for chunk in reader:\n",
    "    review_df = chunk.filter(items=['review_id', 'stars', 'text'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ      2   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA      1   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw      5   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw      1   \n",
       "4  6TdNDKywdbjoTkizeMce8A      4   \n",
       "\n",
       "                                                text  \n",
       "0  As someone who has worked with many museums, I...  \n",
       "1  I am actually horrified this place is still in...  \n",
       "2  I love Deagan's. I do. I really do. The atmosp...  \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...  \n",
       "4  Oh happy day, finally have a Canes near my cas...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>stars</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n      <td>2</td>\n      <td>As someone who has worked with many museums, I...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n      <td>1</td>\n      <td>I am actually horrified this place is still in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n      <td>5</td>\n      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n      <td>1</td>\n      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6TdNDKywdbjoTkizeMce8A</td>\n      <td>4</td>\n      <td>Oh happy day, finally have a Canes near my cas...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                review_id  stars  \\\n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA      1   \n",
       "5  L2O_INwlrRuoX05KSjc4eg      5   \n",
       "6  ZayJ1zWyWgY9S_TRLT_y9Q      5   \n",
       "\n",
       "                                                text  \n",
       "1  I am actually horrified this place is still in...  \n",
       "5  This is definitely my favorite fast food sub s...  \n",
       "6  Really good place with simple decor, amazing f...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>stars</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n      <td>1</td>\n      <td>I am actually horrified this place is still in...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>L2O_INwlrRuoX05KSjc4eg</td>\n      <td>5</td>\n      <td>This is definitely my favorite fast food sub s...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ZayJ1zWyWgY9S_TRLT_y9Q</td>\n      <td>5</td>\n      <td>Really good place with simple decor, amazing f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "review_df.iloc[[1,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize reviews function\n",
    "snowball = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [snowball.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize reviews function\n",
    "def vectorize_reviews(reviews):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', tokenizer = tokenize, max_features = 1000)\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    return X, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(company_id, K = 5, num_words = 20):\n",
    "    labels_color_map = {\n",
    "        0: '#20b2aa', 1: '#ff7373', 2: '#ffe4e1', 3: '#005073', 4: '#4d0404',\n",
    "        5: '#ccc0ba', 6: '#4700f9', 7: '#f6f900', 8: '#00f91d', 9: '#da8c49'\n",
    "    }\n",
    "    pca_num_components = 2\n",
    "    company_df = review_df\n",
    "    # company_name = company_df['name'].unique()[0]\n",
    "    reviews = company_df['text'].values\n",
    "    # print(len(reviews))\n",
    "    # print(reviews)\n",
    "\n",
    "    X, words = vectorize_reviews(reviews)\n",
    "    denseX = X.todense()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(denseX)\n",
    "    print(denseX.shape)\n",
    "    # kmeans = KMeans(n_clusters = K)\n",
    "    # kmeans.fit(X)\n",
    "    # common_words = kmeans.cluster_centers_.argsort()[:,-1:-num_words-1:-1]\n",
    "    # print(common_words.shape)\n",
    "    # print(kmeans.labels_)\n",
    "\n",
    "    reduced_data = PCA(n_components=3).fit_transform(denseX)\n",
    "    fig, ax = plt.subplots()\n",
    "    for index, instance in enumerate(reduced_data):\n",
    "        # print instance, index, labels[index]\n",
    "        pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "        # color = labels_color_map[kmeans.labels_[index]]\n",
    "        color = labels_color_map[reviews.iloc[index][1]]\n",
    "        ax.scatter(pca_comp_1, pca_comp_2, c=color) \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # mydict = {i: np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "    # # print(mydict)\n",
    "    # # print(kmeans.cluster_centers_.argsort()[:,-1:-num_words-1:-1])\n",
    "    # # print(len(words))\n",
    "    # print('Groups of ' + str(num_words) + ' words typically used together in reviews for ' + \\\n",
    "    #       \"all businesses\")\n",
    "    # print(mydict[0].tolist())\n",
    "    # # print()\n",
    "    # print([company_df.iloc[mydict[key].tolist()][\"stars\"].mean() for key in mydict.keys()])\n",
    "\n",
    "    # for num, centroid in enumerate(common_words):\n",
    "    #     print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n(10000, 1000)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-3b55b4a09d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-25b188fa6232>\u001b[0m in \u001b[0;36mprint_clusters\u001b[0;34m(company_id, K, num_words)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# print instance, index, labels[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mpca_comp_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_comp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# color = labels_color_map[kmeans.labels_[index]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_color_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print_clusters(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}