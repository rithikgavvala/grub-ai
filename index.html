<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Machine Learning Class Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>grub.ai</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Rithik Gavvala, Rahul Rajan, Saloni Shah, and Aakash Gupte</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 CS 4641 Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>


<!-- Goal -->
<h3>Project</h3>
Using content from active platforms, to provide businesses & the general public with more accurate analytics on a restaurant’s rating.
<br><br>
<!-- figure -->
<h3>Summary figure</h3>

<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 600px;" alt="" src="infographic.svg">
</div>

<br><br>
<!-- Introduction -->
<h3>Background</h3>
For our project, we decided to look at the current restaurant rating system- we found out that a restaurant’s rating has a significant impact on its potential revenue. More specifically, a 1-star increase in a restaurant’s Yelp rating can lead to a more than 9% increase in revenue. Due to this impact, we wanted to figure out ways to get a more accurate “5-star review” of a restaurant based on the latest data. Fortunately, social media has provided us a way to gain insight into how people feel about restaurants and menu-items in real-time. Introducing grub.ai, a machine learning system that predicts a restaurant’s review based on real-time social-media sources.

<br><br>
<!-- Approach -->
<h3>Methods</h3>
Data preparation/pre-processing
As mentioned earlier, the first stage in data preparation was to download the Yelp review dataset. This dataset provided a lot of information that was extraneous to our use case. Our next stage was to extract the features that we needed from the JSON dataset. We deemed that the most important features from the dataset that we needed were the “review_id”, “stars”, and “text” from each review. We took this data and placed it into a Pandas dataframe. After we had the Pandas dataframe, we needed to process each review text to remove punctuation and stem each word so that we have clean review text for vectorizing. 
<br><br>
<h4>Vectorizing</h4> 
<h6>Tf-idf Vectorizer</h6>
The purpose behind using Tf-idf as a method for vectorizing text is that it is a statistical approach based on term weighting. We have a large number of reviews and we wanted to determine the important words that carry weight based on star-rating. This method allows us to ignore the words that carry less importance (stopwords). 
<br><br>
This algorithm is efficient in matching words in a query to documents that are relevant to the query. Tf-idf has a historical precedent of returning documents that are highly relevant to a particular query. You can also compute the similarity between two arguments with it. The disadvantage of this approach is that since it is based on the bag-of-words model, it may not capture the position in text, semantics, or co-occurrences. It is only useful as a lexical level feature.
Word2Vec Vectorizer<br><br>
We wanted to try using a neural network based vectorizer. With Word2Vec we had an option to create neural word embeddings with either skip-grams or continuous bag of words. We were drawn to this approach because it forms vectors based on context which could be key to determining sentiment within the thousands of reviews that we have. 
<br><br>
The approach is very intuitive, transforming the unlabeled raw corpus into labeled data, and learning the representation of words in a classification task. The process requires little memory, and needs little preprocessing as the data can be fed in a simple way. The simple mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words. Some disadvantages of this approach is that the sub-linear relationships are not explicitly defined, which means there’s little theoretical support behind the characteristic. Also the model may be difficult to train if the softmax function is used since the size of the vocabulary would be too large. Through approximation algorithms are used to address this issue, other problems can still happen like word vectors not being distributed uniformly, and the vector space not being sufficiently utilized. 
<br><br>
<h6>GloVe Vectorizer</h6>
Our group was drawn to GloVe because it trains on co-occurrence counts of words and uses statistics to produce a meaningful word vector space. With this method, we can capture the meaning of one word embedding with the structure of the entire corpus.
<br><br>
The advantages of this algorithm is that it’s goal is very straightforward. It is just trying to enforce the word vectors to capture sub-linear relationships in the vector space.  This approach also adds some more practical meaning into word vectors by considering the relationship between word pairs rather than between words. This algorithm also gives lower weight to highly frequent word pairs to ensure that meaningless stop words don’t dominate the training period. However, a disadvantage to this algorithm is that the model is trained on the co-occurrence matrix words, which takes a lot of memory for storage. If you change any of the parameters related to the matrix, you would have to reconstruct the matrix again in a very time consuming process.<br><br>

<h6>K-Means Clustering </h6>
Utilizing the different vectorizations, we ran the K-means clustering algorithm multiple times in order to extract the data on JSON objects of reviews from the Yelp dataset. After analysis of the three vectorization methods, we predicted that the GloVe method would be more efficient than the Word2vec, and was far more intuitive than Tf-idf. Therefore, we chose to move forward with running the K-means clustering only on the GloVe and Tf-idf vectorizations. By running K-means on both methods, our goal was to determine which method would develop more distinct clusters. Thus, we would be able to provide more accurate information on the individual cluster’s average star ratings for application in future supervised learning techniques such as sentiment analysis. <br><br>
After running both vectorization methods on the reviews of the Yelp dataset, we needed to find the optimal number of clusters in which each dataset may be clustered. Utilizing the elbow method, we were able to plot the explained variance as a function of the number of clusters in range of K. For K-means clustering using Tf-idf vectorization, we were able to determine the __ is the optimal number of clusters. In addition to that, in regards to GloVe vectorization we found the optimal number of clusters to be __.<br><br>
Following this, we were then able to run the K-means clustering algorithm on the Tf-idf vectorized dataset. As you can see in the table below, for each cluster the most common words and its respective average star rating out of 5 is displayed. Clusters that included more positive language, then had an according high average star rating and vice versa. Looking at the cluster with the highest average star rating, #1, the most common words consisted of “great”, “excel”, “recommend”, etc. However, in the graph below it is evident that the resulting clusters are not quite distinct. Thus, it is possible that this vectorization method might not be producing as accurate results as hoped.  <br><br>
Next, we ran the K-means clustering algorithm on the dataset produced from the GloVe vectorization. In order to retrieve the vectorized dataset on the Yelp reviews, we utilized a GloVe model that was previously trained on the Twitter dataset. From here, after running K-means clustering on this dataset we were able to produce the results shown below. The same trend in regards to the relationship between the most common words and average star rating exists. That being said, the difference in comparison to Tf-idf vectorization clustering is apparent. <br><br>
So following the Glove algorithm, we analyzed the clusters in order to deduce each average star rating, which resulted in the slice of data that we needed for our model. 




<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
In order for our team to consider our project successful there are a couple objectives that we are trying to achieve. The first of which is to develop a model that given any single yelp review is able to accurately predict the rating of it. We can test this because we have the data for each review in the form of a JSON object and we can simply cross-reference our model’s prediction with the rating given in the data. The second objective that our team will be trying to complete would be to achieve high accuracy in terms of sentiment analysis on tweets regarding rating of a restaurant. These are the results that we are trying to achieve.

<br><br>

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="results.png">
</div>
<br><br> -->

<!-- Results -->
<h3>Discussion</h3>
In thinking about the outcome of our project, the best outcome would be to have an accurate model that can predict the rating of any review that we pass into it. And thinking about next steps, our goal is to expand the model into application that gives an accurate rating for restaurants and even specific food items. This true rating system would be far more accurate because, rather than relying on outdated reviews, you can actually have a rating system that is decided based on far more recent information, like twitter posts. This rating system solves the massive problem of having lack of information when looking at restaurants, as well as the common disparity between the opinions stated on the internet, and the current thoughts of people towards these restaurants. And expanding on this idea even more, we could see this idea being implemented within any entertainment and recreational buildings, basically anything that has a review system already in place.

<br><br>

<h3>Next Steps</h3>
In order for our team to consider our project successful there are a couple objectives that we are trying to achieve. The first of which is to develop a model that given any single yelp review is able to accurately predict the rating of it. We can test this because we have the data for each review in the form of a JSON object and we can simply cross-reference our model’s prediction with the rating given in the data. The second objective that our team will be trying to complete would be to achieve high accuracy in terms of sentiment analysis on tweets regarding rating of a restaurant. After using K-means clustering on the different vectorization techniques, we have been able to determine the advantages of GloVe vectorization. This decided vectorization technique can be utilized during the supervised learning portion of the project, mainly for sentiment analysis. These are the results that we are trying to achieve. The information deduced from the unsupervised learning techniques executed aid us in reaching our final objective. 
<!-- Results -->
<h3>References</h3>
<ul>
  <li>
      https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9065812

  </li>
  <li>
      https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7050801
  </li>
  <li>
      https://www.kaggle.com/yelp-dataset/yelp-dataset
  </li>
  <li>
      https://www.kaggle.com/damienbeneschi/krakow-ta-restaurans-data-raw
  </li>
</ul>


<!-- Main Results Figure 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br> -->




  <hr>
  <footer> 
  <p>© You Name Here</p>
  </footer>
</div>
</div>

<br><br>

</body></html>